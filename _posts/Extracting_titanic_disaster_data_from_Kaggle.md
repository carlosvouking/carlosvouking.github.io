
# My titanic disaster journey - Part 0 : Project setup & Data Extraction
***

### (I)- Setting up the data science project template

It is important to have a project structure where the project components are linked, consistent and well-organized. 
I will be using the **[coockiecutter](https://drivendata.github.io/cookiecutter-data-science/#nothing-here-is-binding)**, which suits well projects done in python. The goal here is kind of having on the go an initial structure with folders which will hold our datasets, python scripts and the miscelleanous files for required settings.

To install the **[coockiecutter](https://drivendata.github.io/cookiecutter-data-science/#nothing-here-is-binding)**, switch to a terminal, choose a directory for your project and run the following command: 
> ' cookiecutter https://github.com/drivendata/cookiecutter-data-science '

After following the instructions through which you will be giving a name to your project (mine is called **'thetitanic'**), you end up with such a structure ready to hold the data and components of the project.


```python
%%html 
<img src='basic_cookiecutter.png'>
```


<img src='basic_cookiecutter.png'>


### (II)- Extracting Titanic Disaster data from Kaggle

### A/ Cautions:

- Remember that:
   - You must be logged in before downloading the datasets from [Kaggle](http://kaggle.com).
    - You should not use your credentials in the notebook(you might want to push or publish ypur code to github or any other public repo). 
    - You surely don't want to expose your credentials to the public while publishing your code. 
- So, this is the little trick:
    - You should store your login details in the **'.env'** file generated by the cookiecutter project template.
    - This is done by creating 2 variables ('KAGGLE_USERNAME', 'KAGGLE_PASSWORD') that will hold your kaggle's username and password.
    - rest assured that the .env file will never be added in the version control when pushing your project to the public.
    - To check that, go to the **'.gitignore'** file (in the cookiecutter folder) which contains all the files that you don't want to get published.
    - And indeed the **'.env'** file is listed in there. So when sharing your notebook project, this file shall not be pushed anywhere.

 In order to use the login and password details stored in the '.env' file, I will need the python package **python-dotenv**.


```python
# install the package like this:
!pip install python-dotenv
```

    Requirement already satisfied: python-dotenv in /home/cv-dlbox/anaconda3/lib/python3.6/site-packages (0.9.0)


Once the package installed, it will provide you with 2 functions to import to easily  manipulate our '.env' file:  **find_dotenv()** and **load_dotenv()**
- * find_dotenv(): walk you up autmatically through directories until the **(.env)** file is found;
- * load_dotenv(): load up the entries of the **(.env)** file as environmental variables;


```python
# Import the 2 functions llike this:
from dotenv import load_dotenv, find_dotenv

# find_dotenv helps find the .env file
dotenv_path = find_dotenv()

# once the path to the .env is found, load_dotenv() loads it up
load_dotenv(dotenv_path)
```




    True



Once the content of the .env file is stored in the environment variables, the os library helps read their value.


```python
# You can extract the username you mentionned in the .env'' file like this: (
import os
KAGGLE_USERNAME = os.environ.get("KAGGLE_USERNAME")
print(KAGGLE_USERNAME)
# You can get the password stored in '.env' the same way. 
```

    carlstv


### B/ Downloading Training and Test data.

Now that you know how to store and use critical information within your data science project  without sharing it, you can safely download the training and the test datasets from [Kaggle](http://kaggle.com).
- Through this notebook, you first send your credentials to log into the [Kaggle](http://kaggle.com) website.
- Once properly logged in, you can download the datasets.
   * You should create a login session in order to login. The function ***'session()'*** from the python package ***requests*** will help in that. 


```python
# import the package 'requests' like this:
import requests
# import the session() function from the 'requests' library that you have just installed
from requests import session
```


```python
# install the package like this:
!pip install python-dotenv
```

    Requirement already satisfied: python-dotenv in /home/cv-dlbox/anaconda3/lib/python3.6/site-packages (0.9.0)



```python
import os
get_raw_data_script_file = os.path.join(os.path.pardir,'src', 'data', 'get_raw_data.py')
```


```python
%%writefile $get_raw_data_script_file
# ------------content of the 'get_raw_data.py' file------------
import os
from dotenv import find_dotenv, load_dotenv
from requests import session
import logging   #to log intermediate messages to check the steps completed


#credentials for login into kaggle
KAGGLE_USERNAME = os.environ.get('KAGGLE_USERNAME')
KAGGLE_PASSWORD = os.environ.get('KAGGLE_PASSWORD')
credentials_load = {
    'action' : 'login',
    'username' : KAGGLE_USERNAME,
    'password' : KAGGLE_PASSWORD
}


# method to download the data from kaggle and write those to files.
def download_data(url, file_path):
    '''
    method to extract data
    '''
    with session() as ses:
        ses.post('https://www.kaggle.com/account/login', data=credentials_load)
        with open(file_path, 'wb') as handle:
            response = ses.get(url, stream=True)
            for block in response.iter_content(1024):
                handle.write(block)
                
                
# entry point when executing the script.               
def main(project_dir):
    '''
    main method -- entry point of the script.
    '''
    # get the logger
    logger = logging.getLogger(__name__)
    logger.info('Getting raw data from kaggle...')
    
    # urls links to datasets
    train_url = 'https://www.kaggle.com/c/titanic/download/train.csv'
    test_url = 'https://www.kaggle.com/c/titanic/download/test.csv'
    
    # files path
    raw_data_path = os.path.join(project_dir, 'data', 'raw')
    train_data_path = os.path.join(raw_data_path, 'train.csv')
    test_data_path = os.path.join(raw_data_path, 'test.csv')    
    
    # download training data
    download_data(train_url, train_data_path)
    logger.info(f'Raw training data downloaded and saved to: {train_data_path} !')
    # download test data
    download_data(test_url, test_data_path)
    logger.info(f'Raw test data downloaded and saved to: {test_data_path} !')
    
    
    
if __name__ == '__main__':
    # getting the root directory
    #---- my script file 'get_raw_data.py' is 2 levels down from the root folder of our project 'thetitanic'
    project_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) 
    
    #set up logger
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(level=logging.INFO, format=log_format)
    
    # To load the credentials stored in the it, I walk up directories until I find the '.env' file.
    # dotenv_path = find_dotenv()
    # once the '.env' found, load up its content as environmental variables
    # load_dotenv(dotenv_path)    
    
    # Call the main() method
    main(project_dir)        
```

    Overwriting ../src/data/get_raw_data.py



```python
print(os.environ.get('KAGGLE_USERNAME'))
```

    carlstv



```python
!python $get_raw_data_script_file
```

    2018-08-12 17:04:01,362 - __main__ - INFO - Getting raw data from kaggle...
    2018-08-12 17:04:02,803 - __main__ - INFO - Raw training data downloaded and saved to: ../src/data/../../data/raw/train.csv !
    2018-08-12 17:04:04,035 - __main__ - INFO - Raw test data downloaded and saved to: ../src/data/../../data/raw/test.csv !

